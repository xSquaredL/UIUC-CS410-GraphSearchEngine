Lecture 3.5 discusses the specific smoothing methods for language models used for query likelihood retrieval method. The first smoothing method is linear interpolation (Jelinek-Mercer) smoothing method. The idea is to maximize the probability of the observed text. If a word is not observed, we relies on the collection language model where this word will not have zero probability to help us decide what non-zero probability should be assign to this word. The second smoothing method is Dirichlet Prior (Bayesian) smoothing method. Jelinek-Mercer smoothing method uses fixed coefficient linear interpolation and Dirichlet Prior smoothing method adds pseudo counts and adaptive interpolation. They both lead to state of the art retrieval functions with assumptions clearly articulated. Lecture 3.6 talks about feedback in text retrieval.  Relevance feedback is users make explicit relevance judgments on the initial results. Pseudo feedback assumes top-k initial results are relevant. Implicit feedback assumes user-clicked docs are relevant and skipped ones are non-relevant. Lecture 3.7 talks about rRocchio feedback, which is feedback in vector space model. 